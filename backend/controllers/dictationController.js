const OpenAI = require('openai');
const { getInitialExtraction } = require('../services/aiExtractorService');
const fs = require('fs'); // Necesario para Whisper si se usa createReadStream
const os = require('os');
const path = require('path');


// Carga la clave API desde .env (aseg√∫rate que est√© configurado)
let openai;

function getOpenAIClient() {
    if (!openai) {
        if (!process.env.OPENAI_API_KEY) {
            throw new Error('OPENAI_API_KEY is not set in environment variables.');
        }
        openai = new OpenAI();
    }
    return openai;
}

exports.processDictation = async (req, res) => {
    console.log("üé§ Recibida solicitud para procesar dictado...");

    if (!req.file) {
        return res.status(400).json({ message: 'No se recibi√≥ ning√∫n archivo de audio.' });
    }

    let tempFilePath = null;

    try {
        // === 1. Transcribir Audio con Whisper ===
        console.log("Transcribiendo audio con Whisper...");

        // Crear un archivo temporal para enviar a Whisper
        tempFilePath = path.join(os.tmpdir(), `dictation-${Date.now()}.webm`);
        await fs.promises.writeFile(tempFilePath, req.file.buffer);

        const client = getOpenAIClient();
        const transcription = await client.audio.transcriptions.create({
            model: "whisper-1",
            file: fs.createReadStream(tempFilePath), // Enviar como stream
            // language: "es" // Puedes especificar el idioma si siempre ser√° espa√±ol
        });

        // Eliminar archivo temporal despu√©s de usarlo
        try {
            if (tempFilePath && fs.existsSync(tempFilePath)) {
                await fs.promises.unlink(tempFilePath);
            }
        } catch (unlinkErr) {
            console.warn("No se pudo eliminar el archivo de audio temporal:", unlinkErr.message);
        }


        const transcribedText = transcription.text;
        console.log("Texto Transcrito:", transcribedText);

        if (!transcribedText || transcribedText.trim() === '') {
            throw new Error("La transcripci√≥n no produjo texto.");
        }

        // === 2. Extraer Datos con el Servicio Existente ===
        console.log("Extrayendo datos del texto transcrito...");
        // Usamos el servicio existente que ya tiene el prompt adecuado
        const extractedData = await getInitialExtraction(transcribedText);

        // === 3. Enviar Respuesta al Frontend ===
        res.status(200).json(extractedData);

    } catch (error) {
        console.error("‚ùå Error procesando el dictado:", error);
        // Intentar eliminar el archivo temporal tambi√©n en caso de error
        if (tempFilePath && fs.existsSync(tempFilePath)) {
            try { await fs.promises.unlink(tempFilePath); } catch (e) { }
        }
        res.status(500).json({
            message: error.message || 'Error interno al procesar el audio.',
            // Incluir detalles del error de OpenAI si est√°n disponibles
            ...(error.response ? { apiError: error.response.data } : {})
        });
    }
};